{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53541133",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaed12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SpokenDigitDataset import SpokenDigitDataset\n",
    "from utils.DatasetSplitter import DatasetSplitter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52b550",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d8c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset (Total: 30000) into:\n",
      "  Train: 21000 samples (70.0%)\n",
      "  Validation: 4500 samples (15.0%)\n",
      "  Test: 4500 samples (15.0%)\n",
      "Split completo.\n",
      "\n",
      "Accessed train_set (Subset): Size 21000\n",
      "Accessed val_set (Subset): Size 4500\n",
      "Accessed test_set (Subset): Size 4500\n",
      "Configuration complete.\n",
      "\n",
      "Train dataset underlying config after configure_splits: Bilateral=True, Augment=False\n",
      "Validation dataset underlying config after configure_splits: Bilateral=True, Augment=False\n",
      "Test dataset underlying config after configure_splits: Bilateral=True, Augment=False\n",
      "\n",
      "Created Train DataLoader with batch size 32. Number of batches: 657\n",
      "Created Validation DataLoader with batch size 32. Number of batches: 141\n",
      "Created Test DataLoader with batch size 32. Number of batches: 141\n"
     ]
    }
   ],
   "source": [
    "dataset = SpokenDigitDataset(\"data/audio\")\n",
    "\n",
    "# This will automatically perform the split upon creation\n",
    "splitter = DatasetSplitter(\n",
    "        dataset=dataset,\n",
    "        split_ratios=(0.7, 0.15, 0.15) # Example: 70% train, 15% val, 15% test\n",
    ")\n",
    "\n",
    "# You can access the split datasets:\n",
    "train_set = splitter.train_dataset\n",
    "val_set = splitter.val_dataset\n",
    "test_set = splitter.test_dataset\n",
    "\n",
    "print(f\"\\nAccessed train_set (Subset): Size {len(train_set)}\")\n",
    "print(f\"Accessed val_set (Subset): Size {len(val_set)}\")\n",
    "print(f\"Accessed test_set (Subset): Size {len(test_set)}\")\n",
    "\n",
    "# You can configure the underlying dataset for each split\n",
    "# For example, enable augmentation only for the training set\n",
    "splitter.configure_splits(bilateral=True, augment=False)\n",
    "\n",
    "# Check the configuration of the underlying dataset for a split\n",
    "print(f\"\\nTrain dataset underlying config after configure_splits: Bilateral={train_set.dataset.bilateral}, Augment={train_set.dataset.augment}\")\n",
    "print(f\"Validation dataset underlying config after configure_splits: Bilateral={val_set.dataset.bilateral}, Augment={val_set.dataset.augment}\")\n",
    "print(f\"Test dataset underlying config after configure_splits: Bilateral={test_set.dataset.bilateral}, Augment={test_set.dataset.augment}\")\n",
    "\n",
    "\n",
    "# You can access the DataLoaders:\n",
    "train_loader = splitter.train_dataloader\n",
    "val_loader = splitter.val_dataloader\n",
    "test_loader = splitter.test_dataloader\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
