{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53541133",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaed12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SpokenDigitDataset import SpokenDigitDataset\n",
    "from utils.DatasetSplitter import DatasetSplitter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as functional\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import wandb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52b550",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d8c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accessed train_set (Subset): Size 21000\n",
      "Accessed val_set (Subset): Size 4500\n",
      "Accessed test_set (Subset): Size 4500\n"
     ]
    }
   ],
   "source": [
    "dataset = SpokenDigitDataset(\"data/audio\")\n",
    "\n",
    "# This will automatically perform the split upon creation\n",
    "splitter = DatasetSplitter(\n",
    "        dataset=dataset,\n",
    "        split_ratios=(0.7, 0.15, 0.15) # Example: 70% train, 15% val, 15% test\n",
    ")\n",
    "\n",
    "# You can access the split datasets:\n",
    "train_set = splitter.train_dataset\n",
    "val_set = splitter.val_dataset\n",
    "test_set = splitter.test_dataset\n",
    "\n",
    "print(f\"\\nAccessed train_set (Subset): Size {len(train_set)}\")\n",
    "print(f\"Accessed val_set (Subset): Size {len(val_set)}\")\n",
    "print(f\"Accessed test_set (Subset): Size {len(test_set)}\")\n",
    "\n",
    "# You can configure the underlying dataset for each split\n",
    "# For example, enable augmentation only for the training set\n",
    "\n",
    "\n",
    "# Check the configuration of the underlying dataset for a split\n",
    "# print(f\"\\nTrain dataset underlying config after configure_splits: Bilateral={train_set.dataset.bilateral}, Augment={train_set.dataset.augment}\")\n",
    "# print(f\"Validation dataset underlying config after configure_splits: Bilateral={val_set.dataset.bilateral}, Augment={val_set.dataset.augment}\")\n",
    "# print(f\"Test dataset underlying config after configure_splits: Bilateral={test_set.dataset.bilateral}, Augment={test_set.dataset.augment}\")\n",
    "\n",
    "\n",
    "# You can access the DataLoaders:\n",
    "train_loader = splitter.train_dataloader\n",
    "val_loader = splitter.val_dataloader\n",
    "test_loader = splitter.test_dataloader\n",
    "\n",
    "\n",
    "# splitter.configure_splits(bilateral=False, augment=False)\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Raw without augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "splitter.configure_splits(bilateral=True, augment=False)\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Bilateral without Augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# splitter.configure_splits(bilateral=False, augment=True)\n",
    "\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Raw with Augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# splitter.configure_splits(bilateral=True, augment=True)\n",
    "\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Bilateral with Augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affb1bc",
   "metadata": {},
   "source": [
    "### LeNet5 Model: Fitted for 224x224 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea1c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)  # 224 -> 220\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2) # 220 -> 110\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 110 -> 106\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) # 106 -> 53\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5) # 53 -> 49\n",
    "\n",
    "        self.fc1 = nn.Linear(120*49*49, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(functional.tanh(self.conv1(x)))\n",
    "        x = self.pool2(functional.tanh(self.conv2(x)))\n",
    "        x = functional.tanh(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = functional.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d291d406",
   "metadata": {},
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33084cba",
   "metadata": {},
   "source": [
    "### WandB Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1da7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluiscantodd\u001b[0m (\u001b[33mluiscantodd-tec-costa-rica\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/todd/universidad/ia/ai-projects/proyecto-2/wandb/run-20250512_131653-2oymtzhu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist/runs/2oymtzhu' target=\"_blank\">lenet5-audio-run</a></strong> to <a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist' target=\"_blank\">https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist/runs/2oymtzhu' target=\"_blank\">https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist/runs/2oymtzhu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist/runs/2oymtzhu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f812205b4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"Audio-mnist\",\n",
    "    name=\"lenet5-audio-run\",\n",
    "    config={\n",
    "        \"epochs\": 15,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"architecture\": \"LeNet5Audio\",\n",
    "        \"input_size\": \"1x224x224\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351c079",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234f412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data_loader, model, loss_function, optimizer, scheduler):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    losses = []\n",
    "    for inputs, labels in training_data_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = loss_function(out, labels)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct += (out.argmax(1) == labels).sum().item()\n",
    "        \n",
    "\n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    mean_acc = correct/len(training_data_loader.dataset) * 100.\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    return mean_loss, mean_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529355d4",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8745c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validation_data_loader, model, loss_function):\n",
    "    with torch.no_grad():\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        for inputs, labels in validation_data_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            out = model(inputs)\n",
    "            loss = loss_function(out, labels)\n",
    "            losses.append(loss)\n",
    "            correct += (out.argmax(1) == labels).sum().item()\n",
    "        \n",
    "        mean_loss = sum(losses)/len(losses)\n",
    "        mean_acc = correct/len(validation_data_loader.dataset) * 100.\n",
    "\n",
    "        return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ea77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.3257, Validation Loss= 2.3063459396362305  (False), Accuracy=94.77%, Validation Accuracy=9.8%\n",
      "Epoch 2: Loss=0.0814, Validation Loss= 0.11994487047195435  (False), Accuracy=99.05%, Validation Accuracy=98.46666666666667%\n",
      "Epoch 3: Loss=0.0480, Validation Loss= 0.062395431101322174  (False), Accuracy=99.41%, Validation Accuracy=99.22222222222223%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LeNet5(num_classes=10).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    val_loss, val_acc = validate(val_loader, model, loss_function)\n",
    "    mean_loss, mean_acc = train(train_loader, model, loss_function, optimizer, scheduler)\n",
    "        \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train/loss\": mean_loss,\n",
    "        \"train/accuracy\": mean_acc,\n",
    "        \"validation/loss\": val_loss\n",
    "    })\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss={mean_loss:.4f}, Validation Loss= {val_loss}  ({(val_loss > mean_acc)}), Accuracy={mean_acc:.2f}%, Validation Accuracy={val_acc}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
