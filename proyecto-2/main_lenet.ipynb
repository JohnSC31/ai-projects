{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53541133",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaed12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SpokenDigitDataset import SpokenDigitDataset\n",
    "from utils.DatasetSplitter import DatasetSplitter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as functional\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import wandb\n",
    "from utils.Metrics import multiclass_f1_score, multiclass_confusion_matrix\n",
    "from models.lenet import LeNet5\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52b550",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d8c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accessed train_set (Subset): Size 21000\n",
      "Accessed val_set (Subset): Size 4500\n",
      "Accessed test_set (Subset): Size 4500\n"
     ]
    }
   ],
   "source": [
    "dataset = SpokenDigitDataset(\"data/audio\")\n",
    "\n",
    "# This will automatically perform the split upon creation\n",
    "splitter = DatasetSplitter(\n",
    "        dataset=dataset,\n",
    "        split_ratios=(0.7, 0.15, 0.15) # Example: 70% train, 15% val, 15% test\n",
    ")\n",
    "\n",
    "# You can access the split datasets:\n",
    "train_set = splitter.train_dataset\n",
    "val_set = splitter.val_dataset\n",
    "test_set = splitter.test_dataset\n",
    "\n",
    "print(f\"\\nAccessed train_set (Subset): Size {len(train_set)}\")\n",
    "print(f\"Accessed val_set (Subset): Size {len(val_set)}\")\n",
    "print(f\"Accessed test_set (Subset): Size {len(test_set)}\")\n",
    "\n",
    "# You can configure the underlying dataset for each split\n",
    "# For example, enable augmentation only for the training set\n",
    "\n",
    "\n",
    "# Check the configuration of the underlying dataset for a split\n",
    "# print(f\"\\nTrain dataset underlying config after configure_splits: Bilateral={train_set.dataset.bilateral}, Augment={train_set.dataset.augment}\")\n",
    "# print(f\"Validation dataset underlying config after configure_splits: Bilateral={val_set.dataset.bilateral}, Augment={val_set.dataset.augment}\")\n",
    "# print(f\"Test dataset underlying config after configure_splits: Bilateral={test_set.dataset.bilateral}, Augment={test_set.dataset.augment}\")\n",
    "\n",
    "\n",
    "# You can access the DataLoaders:\n",
    "train_loader = splitter.train_dataloader\n",
    "val_loader = splitter.val_dataloader\n",
    "test_loader = splitter.test_dataloader\n",
    "\n",
    "\n",
    "# splitter.configure_splits(bilateral=False, augment=False)\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Raw without augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# splitter.configure_splits(bilateral=True, augment=False)\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Bilateral without Augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# splitter.configure_splits(bilateral=False, augment=True)\n",
    "\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Raw with Augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# splitter.configure_splits(bilateral=True, augment=True)\n",
    "\n",
    "# x, y = train_set[0]\n",
    "# print(\"Etiqueta:\", y)\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# librosa.display.specshow(x.squeeze().numpy(), sr=16000, x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.title('Log-Mel Spectrogram: Bilateral with Augmentation')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d291d406",
   "metadata": {},
   "source": [
    "## Modelo A: LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea3182",
   "metadata": {},
   "source": [
    "#### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d34e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351c079",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data_loader, model, loss_function, optimizer, scheduler, num_classes=10):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total_f1 = 0\n",
    "    losses = []\n",
    "    for inputs, labels in training_data_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = loss_function(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss)\n",
    "        total_f1 += multiclass_f1_score(out, labels, num_classes=num_classes)\n",
    "        correct += (out.argmax(1) == labels).sum().item()\n",
    "        \n",
    "        \n",
    "\n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    mean_acc = correct/len(training_data_loader.dataset) * 100.\n",
    "    mean_f1 = total_f1/len(training_data_loader.dataset)\n",
    "\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    return mean_loss, mean_acc, mean_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529355d4",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8745c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validation_data_loader, model, loss_function, num_classes=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total_f1 = 0\n",
    "        losses = []\n",
    "        for inputs, labels in validation_data_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            out = model(inputs)\n",
    "            loss = loss_function(out, labels)\n",
    "            losses.append(loss)\n",
    "            total_f1 += multiclass_f1_score(out, labels, num_classes=num_classes)\n",
    "            correct += (out.argmax(1) == labels).sum().item()\n",
    "        \n",
    "        mean_loss = sum(losses)/len(losses)\n",
    "        mean_acc = correct/len(validation_data_loader.dataset) * 100.\n",
    "        mean_f1 = total_f1/len(validation_data_loader.dataset)\n",
    "\n",
    "        return mean_loss, mean_acc, mean_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0835fca",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0f4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data_loader, model, num_classes=10):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    conf_matrix = None\n",
    "    total_f1 = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_data_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            out = model(inputs)\n",
    "            correct += (out.argmax(1) == labels).sum().item()\n",
    "            total_f1 += multiclass_f1_score(out, labels, num_classes=num_classes)\n",
    "            if conf_matrix!=None:\n",
    "                conf_matrix = conf_matrix + multiclass_confusion_matrix(out, labels, num_classes=num_classes)\n",
    "            else:\n",
    "                conf_matrix = multiclass_confusion_matrix(out, labels, num_classes=num_classes)\n",
    "\n",
    "        mean_acc = correct/len(test_data_loader.dataset) * 100.\n",
    "        mean_f1 = total_f1/len(test_data_loader.dataset)\n",
    "        return conf_matrix, mean_acc, mean_f1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e9eea",
   "metadata": {},
   "source": [
    "#### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1dec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lenet_model(epochs, lr, splitter, train_loader, val_loader, test_loader, bilateral=False, augmentation=False):\n",
    "    wandb.init(\n",
    "        project=\"Audio-mnist\",\n",
    "        name=f\"lenet5-audio-{'No' if not bilateral else ''}Bi-{'No' if not augmentation else ''}Aug-run\",\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": 32,\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"LeNet5Audio\",\n",
    "            \"input_size\": \"1x224x224\",\n",
    "            \"num_classes\": 10,\n",
    "            \"bilateral_filter\": bilateral,\n",
    "            \"augmentation\": augmentation\n",
    "        }\n",
    "    )\n",
    "    splitter.configure_splits(wandb.config.bilateral_filter, wandb.config.augmentation)\n",
    "\n",
    "    model = LeNet5(wandb.config.num_classes).to(device)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_acc, val_f1 = validate(val_loader, model, loss_function)\n",
    "        # val_loss, val_acc, val_f1 = 0, 0, 0\n",
    "        mean_loss, mean_acc, mean_f1 = train(train_loader, model, loss_function, optimizer, scheduler)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/loss\": mean_loss,\n",
    "            \"train/accuracy\": mean_acc,\n",
    "            \"train/f1\": mean_f1,\n",
    "            \"validation/loss\": val_loss,\n",
    "            \"validation/accuracy\": val_acc,\n",
    "            \"validation/f1\": val_f1\n",
    "        })\n",
    "\n",
    "\n",
    "        print(f\"Epoch #{epoch+1} ({(time.time()-start_time):.1f}s):\\n\\tLoss={mean_loss:.4f}, Val_Loss={val_loss:.4f}, Acc={mean_acc:.2f}%, Val_Acc={val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    conf_matrix, test_acc, test_f1 = test(test_data_loader=test_loader, model=model)\n",
    "\n",
    "    wandb.log({\n",
    "        \"confusion_matrix\":  conf_matrix,\n",
    "        \"test/accuracy\": test_acc,\n",
    "        \"test/f1_score\": test_f1\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06173b5c",
   "metadata": {},
   "source": [
    "### No Augmentation and No Bilateral Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bef76e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluiscantodd\u001b[0m (\u001b[33mluiscantodd-tec-costa-rica\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/todd/universidad/ia/ai-projects/proyecto-2/wandb/run-20250513_095506-90d1lrwv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist/runs/90d1lrwv' target=\"_blank\">lenet5-audio-NoBi-NoAug-run</a></strong> to <a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist' target=\"_blank\">https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist/runs/90d1lrwv' target=\"_blank\">https://wandb.ai/luiscantodd-tec-costa-rica/Audio-mnist/runs/90d1lrwv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 (1095.2s):\n",
      "\tLoss=0.5873, Val_Loss=0.0000, Acc=86.18%, Val_Acc=0.00%\n",
      "Epoch #2 (1054.1s):\n",
      "\tLoss=0.1243, Val_Loss=0.0000, Acc=98.96%, Val_Acc=0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_lenet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.00004\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbilateral\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrun_lenet_model\u001b[39m\u001b[34m(epochs, lr, splitter, train_loader, val_loader, test_loader, bilateral, augmentation)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# val_loss, val_acc, val_f1 = validate(val_loader, model, loss_function)\u001b[39;00m\n\u001b[32m     26\u001b[39m val_loss, val_acc, val_f1 = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m mean_loss, mean_acc, mean_f1 = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m wandb.log({\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: epoch + \u001b[32m1\u001b[39m,\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain/loss\u001b[39m\u001b[33m\"\u001b[39m: mean_loss,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvalidation/f1\u001b[39m\u001b[33m\"\u001b[39m: val_f1\n\u001b[32m     37\u001b[39m })\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()-start_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mLoss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val_Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%, Val_Acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(training_data_loader, model, loss_function, optimizer, scheduler, num_classes)\u001b[39m\n\u001b[32m      4\u001b[39m total_f1 = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m losses = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     51\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/torch/utils/data/dataset.py:419\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/torch/utils/data/dataset.py:419\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/universidad/ia/ai-projects/proyecto-2/utils/SpokenDigitDataset.py:57\u001b[39m, in \u001b[36mSpokenDigitDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     49\u001b[39m y, sr = librosa.load(filepath, sr=\u001b[38;5;28mself\u001b[39m.sr)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# se aplicar ruido\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# if self.augment:\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# noise = np.random.normal(0, 0.005, size=y.shape)\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# y = y + noise\u001b[39;00m\n\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Extraer log-Mel espectrograma\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m mel_spec = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_mels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n\u001b[32m     59\u001b[39m log_mel_spec = log_mel_spec.astype(np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/librosa/feature/spectral.py:2150\u001b[39m, in \u001b[36mmelspectrogram\u001b[39m\u001b[34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[39m\n\u001b[32m   2147\u001b[39m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[32m   2148\u001b[39m mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2150\u001b[39m melspec: np.ndarray = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m...ft,mf->...mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/numpy/_core/einsumfunc.py:1477\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(out, optimize, *operands, **kwargs)\u001b[39m\n\u001b[32m   1474\u001b[39m     right_pos.append(input_right.find(s))\n\u001b[32m   1476\u001b[39m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1477\u001b[39m new_view = \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1479\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1481\u001b[39m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result != results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.anaconda3/envs/proyecto2/lib/python3.11/site-packages/numpy/_core/numeric.py:1177\u001b[39m, in \u001b[36mtensordot\u001b[39m\u001b[34m(a, b, axes)\u001b[39m\n\u001b[32m   1175\u001b[39m at = a.transpose(newaxes_a).reshape(newshape_a)\n\u001b[32m   1176\u001b[39m bt = b.transpose(newaxes_b).reshape(newshape_b)\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m res = \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res.reshape(olda + oldb)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_lenet_model(15, 0.00004, splitter, train_loader, val_loader, test_loader, bilateral=False, augmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee3395",
   "metadata": {},
   "source": [
    "### Augmentation and No Bilateral Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lenet_model(18, 0.0000145, splitter, train_loader, val_loader, test_loader, bilateral=False, augmentation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39167b18",
   "metadata": {},
   "source": [
    "### No Augmentation and Bilateral Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c17ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lenet_model(15, 0.0001, splitter, train_loader, val_loader, test_loader, bilateral=True, augmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728509b4",
   "metadata": {},
   "source": [
    "### Augmentation and Bilateral Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0302da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lenet_model(15, 0.00008, splitter, train_loader, val_loader, test_loader, bilateral=True, augmentation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
